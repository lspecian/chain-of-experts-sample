Deploying and Monitoring Scalable "Chain of Experts" LLM Applications Across Multi-Cloud Environments with Terraform and Langfuse

1. Introduction: The Rise of Complex LLM Chains and the Need for Robust Deployment and Observability
Large Language Models (LLMs) are increasingly moving beyond simple request-response interactions towards more sophisticated architectures involving multiple steps, specialized components, and complex control flows. One such emerging pattern is the "Chain of Experts" (CoE), a technique designed to enhance the capabilities of Mixture-of-Experts (MoE) models by enabling sequential communication between specialized sub-networks within the model's layers. Unlike traditional MoE models where experts often operate in parallel and isolation, CoE introduces an iterative process where the output of one expert can inform the input of the next within the same layer, potentially leading to improved performance and resource efficiency.

However, deploying and managing such complex, stateful LLM applications like CoE models in production presents significant challenges. Developers require robust infrastructure that can scale reliably across different cloud environments, along with sophisticated observability tools to monitor performance, track costs, debug issues, and evaluate the quality of each step in the chain. The non-deterministic nature of LLMs and the intricate dependencies within chained calls make traditional monitoring approaches insufficient.

This report provides a technical roadmap for deploying a "Chain of Experts" application, implemented in TypeScript, to multiple major cloud platforms (AWS, Azure, GCP) in a scalable and consistent manner using Terraform for Infrastructure as Code (IaC). Furthermore, it details how to integrate Langfuse, an open-source LLM engineering platform, to gain deep observability into the CoE application's performance, trace the execution flow of individual experts, and evaluate the quality of both intermediate steps and the final output. The focus is on providing practical guidance, architectural patterns, and code examples to enable teams to build, deploy, and monitor these advanced LLM applications effectively.
2. Understanding the "Chain of Experts" (CoE) Architecture
To appreciate the deployment and monitoring strategies outlined later, it's essential to understand the CoE architecture and how it differs from related concepts like standard Mixture-of-Experts (MoE) and Chain-of-Thought (CoT) or Chain-of-X (CoX) prompting.

2.1. Mixture-of-Experts (MoE) Fundamentals
MoE is a neural network architecture pattern where certain layers, typically the Feed-Forward Networks (FFNs) in Transformer models, are replaced by multiple smaller "expert" subnetworks. A gating network, or router, dynamically selects a subset of these experts to process each input token.

Dense vs. Sparse MoE: In dense MoE, all experts process the input, and their outputs are combined (e.g., weighted average). This increases capacity but also computational cost significantly. Sparse MoE, which is more common in recent LLMs like Mixtral 8x7B, activates only a small subset of experts (e.g., top-1 or top-2) for each token, as determined by the gating network. This allows for a dramatic increase in model parameters (capacity) while keeping the computational cost (FLOPs) relatively constant compared to a smaller dense model.

Gating Mechanism: The router typically calculates relevance scores for each expert based on the input token and selects the top-k experts. Load balancing mechanisms are often employed during training to ensure experts are utilized relatively evenly.

Benefits: MoE enables the training of much larger models under fixed computational budgets, leading to increased model capacity and potentially better performance.

2.2. Chain-of-Experts (CoE): Sequential Communication within MoE
CoE introduces a fundamental change to the sparse MoE paradigm by enabling sequential communication between experts within the same layer. Instead of experts processing tokens independently and in parallel, CoE implements an iterative mechanism where experts can process tokens based on the outputs from other experts within that layer.

Iterative Processing: A CoE layer might involve multiple iterations. In each iteration, a subset of experts is selected (potentially by an independent gating mechanism for each iteration) to process the token's representation, which might be the output from the previous iteration.

Communication: This iterative refinement allows experts to effectively "communicate" and build upon each other's intermediate results before the final output of the MoE layer is produced.

Potential Advantages: Research suggests CoE can outperform standard MoE with similar compute and memory budgets, potentially reducing validation loss and memory requirements. It also significantly increases the number of possible expert processing pathways for each token.

2.3. Distinguishing CoE from CoT/CoX
It's crucial not to confuse the CoE architecture with Chain-of-Thought (CoT) or the broader category of Chain-of-X (CoX) prompting techniques or frameworks.

CoT/CoX: These methods generally involve structuring the input prompt or the generation process to guide an LLM through intermediate steps (thoughts, verifications, tool use, feedback, etc.) to solve complex problems. CoT prompts the LLM to "think step by step". CoX generalizes this to chains of instructions, retrievals, feedback, or even multiple models collaborating at a higher level (e.g., one model plans, another executes).

CoE: This is a specific modification to the internal architecture of an MoE layer within a single LLM, focusing on how expert subnetworks interact during the forward pass for a given token.

While a CoE model could be used within a CoX framework (e.g., a CoE model acting as one "expert" in a higher-level Chain-of-Models), they address different levels of the problem. This report focuses on deploying and monitoring an application embodying the CoE architectural principle, potentially as a sequence of specialized LLM calls orchestrated externally.
3. Multi-Cloud Deployment Strategy with Terraform
Deploying a "Chain of Experts" application requires a robust, scalable, and consistent infrastructure foundation. Using Terraform, we can define this infrastructure as code, enabling automated provisioning and management across multiple cloud providers (AWS, Azure, GCP). This multi-cloud approach mitigates vendor lock-in, enhances disaster recovery capabilities, and allows leveraging best-of-breed services from each provider.

3.1. Project Structure and Organization
A well-organized Terraform project structure is crucial for maintainability and scalability, especially in multi-cloud scenarios. A recommended structure separates concerns like modules, environment configurations, and provider definitions:

.
├── README.md
├── modules/
│   └── app_service/      # Reusable module for the core application service (ECS/App Service/Cloud Run)
│       ├── main.tf
│       ├── variables.tf
│       └── outputs.tf
├── environments/
│   ├── dev/
│   │   ├── main.tf         # Instantiates modules for the dev environment
│   │   ├── terraform.tfvars # Environment-specific variable values (e.g., instance counts, regions)
│   │   └── backend.tf      # Backend configuration for dev state
│   └── prod/
│       ├── main.tf         # Instantiates modules for the prod environment
│       ├── terraform.tfvars # Environment-specific variable values
│       └── backend.tf      # Backend configuration for prod state
├── providers/
│   ├── aws.tf            # AWS provider configuration
│   ├── azure.tf          # Azure provider configuration
│   └── gcp.tf            # GCP provider configuration
├── main.tf               # Root configuration (optional, can orchestrate environments)
└── variables.tf          # Global variables (optional)

modules/: Contains reusable infrastructure components (e.g., VPC, Kubernetes cluster, application service). Each module has its own main.tf (resources), variables.tf (inputs), and outputs.tf (outputs).
environments/: Contains configurations for specific deployment environments (dev, staging, prod). Each environment directory typically has a main.tf to call the reusable modules with environment-specific parameters defined in terraform.tfvars. It also includes the backend configuration (backend.tf) specific to that environment's state file.
providers/: Centralizes provider configurations, ensuring consistency across environments.
main.tf (root): Defines the overall infrastructure composition, potentially calling environment configurations.
variables.tf (root): Defines input variables for the root configuration.
outputs.tf (root): Defines outputs exposed by the root configuration.

3.2. Terraform Provider Configuration
Terraform interacts with cloud platforms via providers. Explicitly configuring and pinning provider versions is essential for stability.

AWS (providers/aws.tf):
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0" # Pin to a specific major version range
    }
  }
}

provider "aws" {
  region = var.aws_region # Define region via variable
  # Credentials typically configured via environment variables or IAM roles
}

Azure (providers/azure.tf):
terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0" # Pin to a specific major version range
    }
  }
}

provider "azurerm" {
  features {}
  # Credentials typically configured via Azure CLI login, Service Principal, or Managed Identity
}

GCP (providers/gcp.tf):
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0" # Pin to a specific major version range
    }
  }
}

provider "google" {
  project = var.gcp_project_id # Define project ID via variable
  region  = var.gcp_region     # Define region via variable
  # Credentials typically configured via gcloud auth application-default login or Service Account key
}
3.3. Dockerizing the TypeScript Application
Containerizing the TypeScript Node.js application using Docker ensures consistency across development, testing, and production environments, regardless of the underlying cloud infrastructure. A multi-stage Dockerfile optimizes the final image size by separating build dependencies from runtime dependencies.

Dockerfile:
# Stage 1: Build Stage
FROM node:20-alpine AS builder
WORKDIR /app

# Copy package files and install dependencies
COPY package*.json ./
RUN npm ci --only=production

# Copy application source code
COPY . .

# Compile TypeScript to JavaScript
RUN npm run build # Assuming a "build" script in package.json (e.g., "tsc")

# Stage 2: Production Stage
FROM node:20-alpine
WORKDIR /app

# Copy only necessary production dependencies and built code from builder stage
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist # Assuming build output is in 'dist'
COPY package.json .

# Expose the port the app runs on
EXPOSE 8080

# Set non-root user for security
USER node

# Command to run the application
CMD ["node", "dist/server.js"] # Adjust path as needed

A .dockerignore file should be used to exclude unnecessary files (like local node_modules, .git, .env) from the build context, keeping the image lean.

3.4. Core Infrastructure Resources (Conceptual HCL)
Terraform code defines the necessary cloud resources. Below are conceptual examples for key components across AWS, Azure, and GCP, focusing on container deployment.

Networking (Conceptual):
A foundational VPC (AWS), VNet (Azure), or VPC Network (GCP) with appropriate subnets (public/private) and security groups/network security groups/firewall rules is required. Using established Terraform modules like terraform-aws-modules/vpc/aws is recommended for best practices.

Container Registry (Conceptual):
Resources like aws_ecr_repository, azurerm_container_registry, or google_artifact_registry_repository are needed to store the Docker images built in the previous step.

Compute/Orchestration (Conceptual):
AWS ECS (Fargate):
# ECS Task Definition (Simplified)
resource "aws_ecs_task_definition" "app_task" {
  family                   = "coe-app-task"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "1024" # Example CPU units
  memory                   = "2048" # Example Memory MiB
  execution_role_arn       = aws_iam_role.ecs_task_execution_role.arn
  container_definitions = jsonencode([{
    name      = "coe-app-container"
    image     = "${aws_ecr_repository.app_repo.repository_url}:latest"
    essential = true
    portMappings = [{
      containerPort = 8080
      hostPort      = 8080
      protocol      = "tcp"
    }]
    #... log configuration, environment variables...
  }])
}

# ECS Service (Simplified)
resource "aws_ecs_service" "app_service" {
  name            = "coe-app-service"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.app_task.arn
  launch_type     = "FARGATE"
  desired_count   = var.service_desired_count # e.g., 2 for HA
  load_balancer {
    target_group_arn = aws_lb_target_group.app_tg.arn
    container_name   = "coe-app-container"
    container_port   = 8080
  }
  network_configuration {
    subnets         = var.private_subnet_ids
    security_groups = [aws_security_group.ecs_service_sg.id]
  }
  #... deployment controller, health check grace period...
}

Azure App Service for Containers:
# App Service Plan (Linux, Premium SKU recommended for containers)
resource "azurerm_service_plan" "app_plan" {
  name                = "coe-app-service-plan"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_resource_group.main.location
  os_type             = "Linux"
  sku_name            = "P1v2" # Example Premium SKU
}

# Linux Web App for Containers (Simplified)
resource "azurerm_linux_web_app" "app_service" {
  name                = "coe-app-service-${random_string.suffix.result}"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_service_plan.app_plan.location
  service_plan_id     = azurerm_service_plan.app_plan.id

  site_config {
    always_on        = true # Keep instances warm
    application_stack {
       docker_image     = "${var.acr_login_server}/${var.docker_image_name}:${var.image_tag}" # Image from ACR
       docker_registry_url = "https://${var.acr_login_server}"
    }
  }

  app_settings = {
    # Required for App Service to know which port the container exposes
    "WEBSITES_PORT" = "8080"
    # ACR credentials if private (use Managed Identity preferably)
    "DOCKER_REGISTRY_SERVER_URL"      = "https://${var.acr_login_server}"
    "DOCKER_REGISTRY_SERVER_USERNAME" = var.acr_admin_username # Or Managed Identity
    "DOCKER_REGISTRY_SERVER_PASSWORD" = var.acr_admin_password # Or Managed Identity
  }
  #... identity, connection strings, networking...
}

GCP Cloud Run:
# Enable necessary APIs
resource "google_project_service" "run_api" {
  service = "run.googleapis.com"
}
resource "google_project_service" "artifactregistry_api" {
  service = "artifactregistry.googleapis.com"
}
#... other APIs like cloudbuild.googleapis.com...

# Cloud Run Service (Simplified)
resource "google_cloud_run_v2_service" "app_service" {
  name     = "coe-app-service"
  location = var.gcp_region
  project  = var.gcp_project_id

  template {
    containers {
      image = "${var.gcp_region}-docker.pkg.dev/${var.gcp_project_id}/${var.artifact_registry_repo_name}/${var.docker_image_name}:${var.image_tag}" # Image from Artifact Registry
      ports {
        container_port = 8080
      }
      #... resources, env vars, volume mounts...
    }
    #... scaling, vpc_access...
  }

  # Allow unauthenticated access (adjust as needed)
  # depends_on = [google_cloud_run_v2_service_iam_binding.allow_unauthenticated]
}

# IAM binding to allow unauthenticated access (optional)
# resource "google_cloud_run_v2_service_iam_binding" "allow_unauthenticated" {
#   project  = google_cloud_run_v2_service.app_service.project
#   location = google_cloud_run_v2_service.app_service.location
#   name     = google_cloud_run_v2_service.app_service.name
#   role     = "roles/run.invoker"
#   members  = ["allUsers"]
# }

Load Balancing / Ingress (Conceptual):
Resources like aws_lb, aws_lb_listener, aws_lb_target_group (AWS); azurerm_lb, azurerm_public_ip, azurerm_lb_backend_address_pool (Azure); or google_compute_global_forwarding_rule, google_compute_target_http_proxy, google_compute_url_map, google_compute_backend_service (GCP) are needed to expose the containerized service to the internet or internal network securely and reliably. API Gateways (aws_api_gateway_rest_api, azurerm_api_management, google_api_gateway_gateway) can provide additional features like authentication, rate limiting, and routing.
3.5. Essential Multi-Cloud Practices in Terraform
Managing infrastructure across multiple clouds introduces complexities that require specific best practices.

Remote State Management: Storing Terraform state files locally is unsuitable for team collaboration and multi-cloud deployments. It leads to inconsistencies and potential data loss. Using remote backends is crucial.

Why: Centralized state ensures all team members work with the same infrastructure view, prevents conflicts through locking mechanisms, and provides better durability.

How: Configure a backend block in each environment's configuration (environments/dev/backend.tf, environments/prod/backend.tf).

AWS S3 + DynamoDB:
terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket-prod" # Unique bucket per environment/project
    key            = "coe-app/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-state-lock-prod" # DynamoDB table for locking
    encrypt        = true
  }
}

Azure Blob Storage:
terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg-prod"
    storage_account_name = "tfstateprodsa" # Unique storage account
    container_name       = "tfstate"
    key                  = "coe-app.terraform.tfstate"
  }
}

Google Cloud Storage (GCS):
terraform {
  backend "gcs" {
    bucket  = "my-terraform-state-bucket-prod" # Unique bucket
    prefix  = "terraform/state/coe-app"
  }
}

Workspaces: Terraform workspaces allow managing multiple instances of the same configuration (e.g., dev, staging, prod) with separate state files but shared code, reducing duplication.

Secrets Management: Hardcoding sensitive data like API keys, database passwords, or Langfuse credentials directly into Terraform configuration files (.tf or .tfvars) is a major security risk. These files are often committed to version control, exposing secrets.

Why: Avoids accidental exposure in code repositories, facilitates secret rotation, enables centralized auditing, and meets compliance requirements.

How: Integrate with dedicated secret management tools. Avoid relying solely on environment variables for production secrets, as they can still be exposed. Use Terraform data sources or dedicated provider resources to fetch secrets at runtime.

Conceptual Example (using AWS Secrets Manager data source):
data "aws_secretsmanager_secret_version" "langfuse_api_key" {
  secret_id = "prod/langfuse/api-keys" # Name/ARN of the secret in Secrets Manager
}

resource "aws_ecs_task_definition" "app_task" {
  #... other config...
  container_definitions = jsonencode([{
    #... other container config...
    environment = [
      {
        name = "NODE_ENV"
        value = "production"
      },
      {
        name = "LANGFUSE_PUBLIC_KEY"
        value = jsondecode(data.aws_secretsmanager_secret_version.langfuse_api_key.secret_string)["public_key"]
      }
      # Add LANGFUSE_BASEURL if needed
    ]
    # Or use secrets parameter for direct injection (supported by ECS)
    # secrets = [
    #   {
    #     name = "LANGFUSE_SECRET_KEY"
    #     valueFrom = "${data.aws_secretsmanager_secret_version.langfuse_api_key.arn}:secret_key::"
    #   }
    # ]
    #... other container config...
  }])
}

3.6. Infrastructure Choice and Observability Implications
The choice of compute infrastructure—container orchestration (ECS, GKE, Azure Container Apps/AKS) versus serverless functions (Lambda, Azure Functions, Cloud Functions/Cloud Run)—significantly influences the approach needed for effective tracing with Langfuse.

Containerized services typically run as longer-lived processes within a more stable networking environment provided by the orchestrator. This can simplify the automatic propagation of tracing context (like trace IDs) between different parts of the application running within the same task or pod, often handled implicitly by OpenTelemetry instrumentation libraries.

Serverless functions, however, are ephemeral and often stateless by design. Each invocation might run in a new, isolated environment. Standard context propagation mechanisms that rely on in-memory state or thread-local storage might not work reliably across separate function invocations. This necessitates a more explicit approach to trace continuity. Langfuse SDKs provide specific mechanisms for these short-lived environments, such as the flush() method in Python or shutdownAsync() in TypeScript, which ensure that all buffered telemetry data is sent before the function terminates. Failure to use these can result in lost traces. Furthermore, manually passing trace and parent span IDs between serverless function calls (e.g., via event payloads or headers) often becomes necessary to link them correctly within the Langfuse UI.

For this report, the focus is on a container orchestration approach (ECS, App Service/Container Apps, Cloud Run) as it often provides a more straightforward path for managing stateful or long-running processes potentially involved in a complex "Chain of Experts" application, and simplifies the initial setup for tracing context propagation compared to a purely serverless function-based architecture. However, the principles outlined can be adapted for serverless deployments with careful attention to context passing and SDK flushing mechanisms.
4. Instrumenting the Chain with Langfuse & TypeScript
Once the infrastructure is provisioned, the next step is to instrument the TypeScript "Chain of Experts" application to send telemetry data to Langfuse. Langfuse provides SDKs and leverages OpenTelemetry standards to capture detailed traces of LLM application executions.

4.1. Langfuse Tracing Fundamentals
Langfuse structures observability data around a few core concepts:

Trace: Represents a single end-to-end execution of your application or feature (e.g., processing one user request through the CoE chain). Traces have attributes like name, userId, sessionId, tags, and metadata.

Observation: Represents a specific unit of work within a trace. Observations can be nested to show relationships. There are three main types:
- Span: Tracks a time-bound operation (e.g., a call to a specific expert function, a database query). Captures startTime, endTime, input, output, metadata.
- Generation: A specialized type of Span specifically for LLM calls. Captures additional LLM-specific details like model parameters, prompt, completion, usage (tokens), and calculated cost.
- Event: Represents a discrete point-in-time occurrence within a trace (e.g., an error log, a specific state change).

Score: Represents an evaluation metric (numeric, categorical, or boolean) attached to a Trace or a specific Observation. Used for quality assessment, user feedback, or automated evaluations.

Langfuse is designed to be compatible with OpenTelemetry (OTel), the open standard for observability data. Langfuse server accepts OTel protocol data, and the SDKs are moving towards being OTel-native. This alignment means Langfuse fits within a broader observability ecosystem.

4.2. Integrating the Langfuse TypeScript SDK
Integrating Langfuse into a TypeScript application involves installing the SDK, initializing the client, and using its methods to create traces and observations.

Setup:
Installation:
npm install langfuse
# or
yarn add langfuse

Initialization: Create an instance of the Langfuse client early in your application's lifecycle.
import { Langfuse } from "langfuse";

const langfuse = new Langfuse({
  secretKey: process.env.LANGFUSE_SECRET_KEY, // Required
  publicKey: process.env.LANGFUSE_PUBLIC_KEY, // Required
  baseUrl: process.env.LANGFUSE_BASEURL,     // Optional: Defaults to EU Cloud "https://cloud.langfuse.com"
  // release: process.env.APP_RELEASE_VERSION, // Optional: Track deployments
  // flushAt: 10, // Optional: Batch size (default 10)
  // flushInterval: 10, // Optional: Interval in seconds (default 10)
});

Configuration (API Keys & Host): Langfuse credentials (LANGFUSE_SECRET_KEY, LANGFUSE_PUBLIC_KEY) are required. They can be passed directly to the constructor or, preferably for cloud deployments, configured via environment variables. The baseUrl (or LANGFUSE_BASEURL environment variable) is crucial for targeting the correct Langfuse instance:
- EU Cloud: https://cloud.langfuse.com (Default)
- US Cloud: https://us.cloud.langfuse.com
- HIPAA Cloud: https://hipaa.cloud.langfuse.com
- Self-Hosted: Your self-hosted Langfuse URL

Failure to set the correct baseUrl will result in data not reaching your project.

Core Tracing Implementation (TypeScript Snippets):
Assume a CoE application structure where a main function orchestrates calls to different expertFunctions.

Creating a Root Trace: Wrap the entire request handling logic in a trace.
import { v4 as uuidv4 } from 'uuid'; // For generating unique IDs if needed

async function handleRequest(userInput: any, userId: string, sessionId: string) {
  const traceId = uuidv4(); // Optional: Generate your own ID

  const trace = langfuse.trace({
    id: traceId,
    name: "coe-request-handler", // Descriptive name for the trace
    userId: userId,             // Track user associated with the request
    sessionId: sessionId,       // Group traces belonging to the same session/conversation
    metadata: { clientInfo: "web-ui", requestType: "complex-query" },
    tags: ["production", "chain-of-experts"],
  });

  try {
    // Pass the trace object down to expert functions
    const expert1Input = prepareInputForExpert1(userInput);
    const expert1Output = await callExpert1(expert1Input, trace);

    const expert2Input = prepareInputForExpert2(expert1Output);
    const expert2Output = await callExpert2(expert2Input, trace);

    const finalOutput = formatFinalOutput(expert2Output);

    // Update trace with final output
    trace.update({
      output: finalOutput,
      metadata: { processingStatus: "success" }
    });

    return finalOutput;

  } catch (error) {
    trace.update({
       level: "ERROR", // Mark trace level as ERROR
       statusMessage: error instanceof Error? error.message : "Unknown error",
       output: { error: "Processing failed" }
    });
    console.error("CoE processing failed:", error);
    throw error; // Re-throw error
  } finally {
    // Ensure Langfuse client sends remaining data, especially important in short-lived environments
    // await langfuse.shutdownAsync(); // Use this in serverless/edge environments
  }
}
Creating Spans/Generations for Each Expert: Instrument each expert call. If an expert involves an LLM call, use generation; otherwise, use span.
import { Langfuse, LangfuseTraceClient } from "langfuse"; // Import LangfuseTraceClient type
import { OpenAI } from "openai"; // Assuming OpenAI usage

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function callExpert1(input: any, trace: LangfuseTraceClient): Promise<any> {
  // Use trace.span for non-LLM processing steps
  const span = trace.span({
    name: "expert1-data-retrieval",
    input: input,
    metadata: { expertType: "retrieval" },
  });

  try {
    // Simulate data retrieval logic
    const retrievedData = await externalDataService.fetch(input.query);

    span.end({ output: retrievedData }); // Mark span end and record output
    return retrievedData;
  } catch (error) {
    span.end({
      level: "ERROR",
      statusMessage: error instanceof Error? error.message : "Retrieval failed",
    });
    throw error;
  }
}

async function callExpert2(input: any, trace: LangfuseTraceClient): Promise<any> {
  // Use trace.generation for LLM calls
  const generation = trace.generation({
    name: "expert2-summarization",
    input: input.documents, // Can be structured object or array of messages
    model: "gpt-4o",
    modelParameters: { temperature: 0.7, max_tokens: 500 },
    metadata: { expertType: "summarization-llm" },
    // prompt: // Optional: Can be explicitly set if needed, often inferred from input
  });

  try {
    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        { role: "system", content: "You are a summarization expert." },
        { role: "user", content: `Summarize the following documents: ${JSON.stringify(input.documents)}` }
      ],
      temperature: 0.7,
      max_tokens: 500,
    });

    const result = completion.choices[0].message.content;
    const usage = completion.usage; // Includes prompt_tokens, completion_tokens, total_tokens

    generation.end({
      output: result,
      usage: usage, // Langfuse calculates cost based on usage and model
      // completionStartTime: // Optional: Can be set for more precise latency
    });
    return { summary: result };
  } catch (error) {
    generation.end({
      level: "ERROR",
      statusMessage: error instanceof Error? error.message : "Summarization failed",
    });
    throw error;
  }
}

Nesting: Calling .span() or .generation() on the trace object automatically links them as children of the trace. If you needed nested steps within an expert, you could call .span() on the expert's span or generation object. This creates the visual hierarchy in Langfuse.

LLM Details: The trace.generation() method captures key LLM details like model, modelParameters, input (prompt/messages), output (completion), and usage (token counts). Langfuse uses this information, particularly the usage and model, to automatically calculate and display the cost for that generation step.

Adding Context: Use .update() on trace or observation objects to add metadata discovered later in the process.
// Inside callExpert2, after successful summarization
generation.update({ metadata: { summaryLength: result?.length } });
trace.update({ tags: ["summarization-successful"] }); // Add a tag to the parent trace

Handling Asynchronicity: The Langfuse TypeScript SDK sends events asynchronously in batches to avoid blocking your main application thread. This is crucial for performance in Node.js applications.

Serverless/Edge Considerations: In environments like AWS Lambda, Azure Functions, Vercel Edge Functions, or Cloudflare Workers, the execution context can terminate abruptly. It is essential to call await langfuse.shutdownAsync() at the end of your function handler (e.g., in a finally block) to ensure all buffered events in the SDK's internal queue are sent to the Langfuse API before the environment shuts down. Without this, you risk losing telemetry data.

4.3. The Role of OpenTelemetry
Understanding OpenTelemetry (OTel) provides valuable context for using Langfuse effectively. OTel is an industry standard for instrumenting, generating, collecting, and exporting telemetry data (traces, metrics, logs) in a vendor-neutral way.

Langfuse embraces OTel: its server can ingest OTel data directly via OTLP (OpenTelemetry Protocol), and its SDKs are planned to become OTel-native. This means that when you use the Langfuse SDK to create a trace, span, or generation, you are likely creating data structures that align with OTel concepts and its emerging Semantic Conventions for Generative AI.

These GenAI conventions define standard attribute names for LLM-related data, such as:
- gen_ai.system (e.g., "OpenAI", "Anthropic")
- gen_ai.request.model (e.g., "gpt-4o")
- gen_ai.prompt / llm.input_messages
- gen_ai.completion / llm.output_messages
- gen_ai.usage.prompt_tokens / llm.token_count.prompt
- gen_ai.usage.completion_tokens / llm.token_count.completion
- traceloop.span.kind (e.g., "workflow", "task", "agent", "tool")
- llm.function_call

By adhering to or mapping to these conventions, Langfuse ensures that the data it collects is structured meaningfully and can potentially be integrated with other OTel-compliant observability tools. For developers, this means that skills learned with OTel are transferable to Langfuse, and the telemetry captured by Langfuse fits into a standardized observability landscape.
5. Visualizing and Analyzing the Chain in Langfuse
Effective instrumentation is only useful if the collected data can be easily visualized and analyzed. Langfuse provides a UI specifically designed for understanding complex LLM application flows.

5.1. Navigating Traces in the Langfuse UI
The nested trace.span() and trace.generation() calls made in the TypeScript code translate directly into a hierarchical view within the Langfuse UI.

Nested Trace View: Each trace displays a tree-like structure where the root is the main trace object (coe-request-handler in our example), and child nodes represent the spans and generations created within it (expert1-data-retrieval, expert2-summarization). This allows developers to visually follow the execution flow step-by-step. Clicking on any node reveals its details, including input, output, metadata, duration, cost (for generations), and associated scores.

Timeline View: Langfuse offers a timeline or waterfall view for each trace. This visualization shows the duration and sequential execution of each observation (span, generation), making it easy to identify latency bottlenecks within the chain – for example, determining if expert1 or expert2 is taking the longest time.

Agent Graphs: For more complex, potentially non-linear or stateful agentic workflows (which might involve loops or conditional branching beyond a simple sequential chain), Langfuse provides Agent Graph visualizations. These graphs represent the flow between different states or tools used by an agent, offering a higher-level view of the agent's decision-making process. Example traces showcasing these views are often available in Langfuse documentation or demos.

5.2. Monitoring Performance with Dashboards
Langfuse includes dashboarding capabilities to monitor key performance indicators (KPIs) across many traces. While default dashboards provide general metrics, the platform also offers Custom Dashboards (currently in Beta) for creating personalized views.

Users can create widgets to visualize specific metrics derived from trace and observation data. Although the specifics of widget configuration are evolving, the goal is to allow filtering and aggregation based on trace/observation attributes (like names, tags, metadata, user IDs, session IDs) and scores.

For monitoring a "Chain of Experts", the following metrics are particularly relevant and can potentially be visualized in custom dashboards:

Metric Name | Description | Relevance to Chain/Expert | How to Track (Langfuse Feature/SDK)
------------|-------------|---------------------------|--------------------------------------
Trace Latency | Total time taken for the entire chain execution. | Overall chain performance | Calculated automatically for each trace.
Span/Generation Latency | Time taken for an individual expert step (span or generation). | Individual expert performance | Calculated automatically for each observation (span/generation).
Trace Cost | Total calculated cost for all LLM calls within a trace. | Overall chain cost | Aggregated automatically from generation costs within a trace.
Generation Cost | Calculated cost for a single LLM call (expert). | Individual expert cost | Calculated automatically for each generation based on model and token usage (usage field).
Token Usage (Prompt/Compl.) | Number of input/output tokens used by an LLM expert. | Individual expert efficiency/cost | Captured automatically in the usage field of a generation.
Token Usage (Total) | Sum of prompt and completion tokens for an expert or the entire trace. | Expert/Chain efficiency/cost | Captured in usage (generation) or aggregated across generations (trace).
Error Rates | Frequency of traces or specific observations marked with level: "ERROR". | Overall chain / Individual expert reliability | Set level: "ERROR" and statusMessage via SDK (.update() or .end()). Filter/aggregate in UI/API.
User Feedback Scores | Scores (e.g., thumbs up/down, ratings) provided by end-users. | Overall chain quality/satisfaction | Logged via SDK/API (langfuse.score() attached to trace or session).
LLM-as-a-Judge Scores | Automated quality scores (e.g., relevance, correctness) from evaluations. | Step-specific or overall chain quality | Configured via Langfuse UI/API (Evaluators). Scores attached to traces or observations.

This table highlights how Langfuse metrics map directly to performance aspects of the CoE application, enabling targeted monitoring.

5.3. Evaluating Performance with Langfuse Scores
Beyond quantitative metrics like latency and cost, evaluating the quality of LLM outputs is critical. Langfuse's scoring system provides a flexible way to attach qualitative or quantitative assessments to executions.

Attaching Scores: Scores can be associated with either an entire trace or a specific observation (span or generation) within that trace using its ID. Scores have a name (e.g., "hallucination-check", "user-feedback"), a value (numeric, categorical, or boolean), and optional comment.

// Example: Scoring the final trace based on user feedback
await langfuse.score({
  traceId: traceId, // ID of the trace we created earlier
  name: "user-satisfaction",
  value: 1, // e.g., 1 for positive, 0 for negative
  comment: "User found the answer helpful.",
});

// Example: Scoring an individual expert's output (requires observation ID)
// Assume expert2GenerationId was captured when creating the generation
await langfuse.score({
  traceId: traceId,
  observationId: expert2GenerationId, // Link score to the specific generation
  name: "summary-relevance",
  value: 0.9, // e.g., a relevance score from 0 to 1
  comment: "Summary accurately reflects input documents.",
});

Importance of Granular Evaluation: For complex chains like CoE, evaluating only the final output is often insufficient. A poor result might stem from an issue in any preceding step. Langfuse's ability to attach scores to specific observations is therefore crucial. By evaluating each expert's output individually (e.g., scoring the relevance of expert1's retrieval or the correctness of expert2's summary), developers can pinpoint the exact source of problems within the chain. This allows for targeted debugging and refinement of specific prompts, models, or logic within the chain, leading to more robust and reliable overall performance.

LLM-as-a-Judge: Langfuse offers managed LLM-as-a-judge evaluations. These can be configured in the UI to automatically assess traces or specific observations based on predefined templates (e.g., checking for hallucination, relevance, correctness, toxicity) or custom prompts. The judge LLM evaluates the input/output/metadata of the target observation and assigns a score, which is then stored as a Langfuse score object. This allows for scalable, automated quality assessment applied granularly to each step of the CoE chain if needed.
6. Conclusion: Building Observable, Scalable LLM Chains
This report has outlined a comprehensive approach for deploying and monitoring a "Chain of Experts" LLM application, built with TypeScript, across AWS, Azure, and GCP. By leveraging Terraform for Infrastructure as Code, teams can achieve consistent, scalable, and automated multi-cloud deployments, mitigating vendor lock-in and enabling flexibility. Key practices like modular design, remote state management, and secure secret handling are essential for managing the complexity of multi-cloud infrastructure.

Complementing the robust infrastructure is deep observability provided by Langfuse. Instrumenting the TypeScript application with the Langfuse SDK allows for detailed tracing of the entire execution flow, capturing the inputs, outputs, latency, cost, and metadata associated with each "expert" step in the chain. The Langfuse UI, with its nested trace views, timelines, and agent graphs, provides intuitive ways to visualize and debug these complex interactions.

Crucially, Langfuse enables granular performance monitoring and evaluation. Custom dashboards can track key metrics like latency and cost per expert, while the flexible scoring system allows for attaching quality assessments (including user feedback and LLM-as-a-judge evaluations) to individual steps within the chain. This step-level evaluation is vital for identifying bottlenecks and refining specific components within complex LLM workflows, leading to more reliable and effective applications.

By combining the strengths of Terraform for multi-cloud infrastructure automation and Langfuse for LLM-specific observability and evaluation, development teams gain the necessary tools to confidently build, deploy, monitor, and iterate on sophisticated, scalable "Chain of Experts" applications and other complex LLM systems.

Next Steps:
- Explore Langfuse's Prompt Management features to version control and deploy prompts used by the experts without code changes.
- Utilize Langfuse Datasets to create test suites from production edge cases for regression testing and benchmarking new expert versions.
- Investigate advanced evaluation techniques within Langfuse, such as custom evaluators or integrating human annotation workflows.
- Deepen the integration with OpenTelemetry by instrumenting other parts of the application stack (e.g., databases, external APIs) to gain end-to-end visibility beyond the LLM chain itself.